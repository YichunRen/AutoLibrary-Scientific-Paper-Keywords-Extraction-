{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis\n",
    "\n",
    "In this notebook, we perform a result analysis of our model in two aspects:\n",
    "\n",
    "1. whether the model is able to differentiate similar papers published by the same author, while at the same time discovering their shared topics\n",
    "\n",
    "2. whether the model is able to give precise results compared to manual labeling\n",
    "\n",
    "To start, we select 5 papers published by Professor Shang:\n",
    "- CrossWeigh\n",
    "    - Semantic Scholar: https://www.semanticscholar.org/paper/CrossWeigh%3A-Training-Named-Entity-Tagger-from-Wang-Shang/997855e1f17d34dd3922d953a587742d198844e6\n",
    "    - PDF: https://www.aclweb.org/anthology/D19-1519.pdf\n",
    "- AutoPhrase\n",
    "    - Semantic Scholar: https://www.semanticscholar.org/paper/Automated-Phrase-Mining-from-Massive-Text-Corpora-Shang-Liu/96808500be49f3d502055bab1edd30dcbec4b99b\n",
    "    - PDF: http://hanj.cs.illinois.edu/pdf/tkde18_jshang2.pdf\n",
    "- LM-LSTM-CRF\n",
    "    - Semantic Scholar: https://www.semanticscholar.org/paper/Empower-Sequence-Labeling-with-Task-Aware-Neural-Liu-Shang/7647a06965d868a4f6451bef0818994100a142e8\n",
    "    - PDF: https://arxiv.org/pdf/1709.04109.pdf\n",
    "- AutoNER\n",
    "    - Semantic Scholar: https://www.semanticscholar.org/paper/Learning-Named-Entity-Tagger-using-Domain-Specific-Shang-Liu/5201efab94c9376ef894f6f33cab06a5c5e00073\n",
    "    - PDF: https://www.aclweb.org/anthology/D18-1230.pdf\n",
    "- SetExpan\n",
    "    - Semantic Scholar: https://www.semanticscholar.org/paper/SetExpan%3A-Corpus-Based-Set-Expansion-via-Context-Shen-Wu/741d50647afac926dce001160d8253c7a5c14ca3\n",
    "    - PDF: https://arxiv.org/pdf/1910.08192.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir('../references/result_analysis')\n",
    "dirs.remove('.DS_Store')\n",
    "dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ability to compare and contrast similar papers published by the same author\n",
    "\n",
    "First we get the AutoPhrase results for all 5 papers. We select high quality phrases (quality score > 0.5) only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase = {}\n",
    "autophrase_all = {}\n",
    "autophrase_stats = pd.DataFrame()\n",
    "for directory in dirs:\n",
    "    fp = '../references/result_analysis/' + directory + '/AutoPhrase.txt'\n",
    "    df = pd.read_csv(fp, delimiter='\\t', header=None, names=['score', 'phrase'])\n",
    "    df = df[['phrase', 'score']]\n",
    "    autophrase_all[directory] = df\n",
    "    df = df[df['score'] > 0.5]\n",
    "    autophrase[directory] = df\n",
    "    autophrase_stats[directory] = df['score'].describe()\n",
    "autophrase_df = pd.concat(autophrase, axis=1)\n",
    "autophrase_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the dataframe above, there are some phrases, such as \"maccabi tel aviv,\" \"jiawei han,\" and \"california,\" that ended up at the top of the ranked list while they are actually not domain-specific. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter out these nonsignificant phrases, our model applies weight to the AutoPhrase result using the pre-processed arXiv dataset. For pre-processing, we have split the arXiv dataset into domains and run AutoPhrase on each of them to get domain specific phrases. For the 5 papers we are using, we select the domain to be \"computer science\" and the weighted results with high quality phrases (quality score > 0.5) are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weighted = {}\n",
    "weighted_all = {}\n",
    "weighted_stats = pd.DataFrame()\n",
    "for directory in dirs:\n",
    "    fp = '../references/result_analysis/' + directory + '/weighted_AutoPhrase.csv'\n",
    "    df = pd.read_csv(fp, index_col='Unnamed: 0')\n",
    "    weighted_all[directory] = df\n",
    "    df = df[df['score'] > 0.5]\n",
    "    weighted[directory] = df\n",
    "    weighted_stats[directory] = df['score'].describe()\n",
    "weighted_df = pd.concat(weighted, axis=1)\n",
    "weighted_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the dataframe above, there are some phrases, such as \"natural language,\" \"pos tagging,\" \"lstm crf,\" and \"text corpora,\" shared across these 5 papers. At the same time, each of these papers has its own unique phrases, such as \"cross validation\" for CrossWeigh, \"knowledge base\" for AutoPhrase, \"sequence labeling\" for LM-LSTM-CRF, \"distant supervision\" for AutoNER, and \"bipartite graph\" for SetExpan. \n",
    "\n",
    "Thus our model is able to differentiate similar papers published by the same author, while at the same time discovering their shared topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density plots of quality scores before and after applying weight are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "for directory in autophrase_all:\n",
    "    sns.distplot(autophrase_all[directory]['score'].to_list(), hist=False, label=directory)\n",
    "plt.title('AutoPhrase Quality Score Distribution', fontsize=25)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 4)\n",
    "plt.ylabel('Density', fontsize=20)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontname('Arial')\n",
    "    label.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "for directory in weighted_all:\n",
    "    sns.distplot(weighted_all[directory]['score'].to_list(), hist=False, label=directory)\n",
    "plt.title('Weighted Quality Score Distribution', fontsize=25)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 4)\n",
    "plt.ylabel('Density', fontsize=20)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontname('Arial')\n",
    "    label.set_fontsize(15)\n",
    "\n",
    "# plt.savefig('../data/report/Quality Score Distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality score distribution shifts to the left after applying the weight. This is expected because nonsignificant phrases are weighted down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ability to give precise results comparing to manual labeling\n",
    "\n",
    "To do this, we annotate the weighted results by manual checking and labeling whether the phrases can actually represent the paper. We compare the accuracy for phrases with a quality score > 0.5, > 0.6, and > 0.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annote_stats = pd.DataFrame(columns=['Article', \n",
    "                                     'Accuracy (quality score > 0.5)', \n",
    "                                     'Accuracy (quality score > 0.6)', \n",
    "                                     'Accuracy (quality score > 0.7)'])\n",
    "for directory in dirs:\n",
    "    fp = '../references/result_analysis/' + directory + '/annotation.csv'\n",
    "    df = pd.read_csv(fp, index_col='Unnamed: 0')\n",
    "    df2 = df[df['score'] > 0.6]\n",
    "    df3 = df[df['score'] > 0.7]\n",
    "    annote_stats = annote_stats.append({'Article': directory, \n",
    "                                        'Accuracy (quality score > 0.5)': df['label'].mean(), \n",
    "                                        'Accuracy (quality score > 0.6)': df2['label'].mean(),\n",
    "                                        'Accuracy (quality score > 0.7)': df3['label'].mean()}, ignore_index=True)\n",
    "annote_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the dataframe above, accuracy is higher for phrases with a higher quality score. \n",
    "\n",
    "Thus our model is able to give precise results compared to manual labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision-recall curves are plotted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for directory in dirs:\n",
    "    fp = '../references/result_analysis/' + directory + '/annotation.csv'\n",
    "    sample = pd.read_csv(fp, index_col='Unnamed: 0')\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        y_true=sample['label'],\n",
    "        probas_pred=sample['score'])\n",
    "    plt.plot(recall, precision, scalex=False, scaley=False, label=directory)\n",
    "plt.title('Precision-Recall Curve', fontsize=25)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.tick_params(axis='both',labelsize=15)\n",
    "\n",
    "# plt.savefig('../data/report/Precision-Recall Curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
