0.9383894292	web service
0.9269696756	noodle soup
0.8997884219	online analytical processing
0.8991073973	valet parking
0.8972437485	feature extraction
0.8824736311	relational database
0.8792372684	data mining
0.8772234479	machine learning
0.8772078663	gene expression
0.8768392155	data structure
0.8727689774	valet service
0.8545542682	dynamic programming
0.8499455647	data base
0.8262026354	association rule
0.8228637465	data cube
0.7978177256	text mining
0.7860940989	idf
0.7824561708	knowledge discovery
0.7798353222	phrasal segmentation
0.7762707166	yelp
0.7726800982	academia
0.7638316372	hash table
0.7636386902	wiki
0.7584021776	topmine
0.7471656966	sec
0.7451671972	aho corasick automaton
0.7428624825	data set
0.7336259010	vector machine
0.7310899978	∣ ∣ ∣
0.7297193628	semantic unit
0.7282737726	acl
0.7261644916	tf idf
0.7176551190	academia and yelp
0.7174207799	segphrase
0.7167069159	viterbi training
0.7134067043	vector
0.7117794240	cube
0.7064730752	detection
0.7061558714	appendix
0.7061555080	sigmod
0.7061127124	vt
0.7051691741	random
0.7049037741	classphrase
0.7026155173	acm
0.7001309528	alg
0.6992207275	nlp
0.6962722905	free
0.6955024281	sigkdd
0.6936740203	noodles
0.6934999077	noodle
0.6925336324	labels
0.6913048363	knowledge
0.6907477276	sequence
0.6882350712	management
0.6843369163	database
0.6826640837	recall 0 0.2 0.4 0.6 0.8
0.6823997716	noun
0.6810362907	segphrase + chunking
0.6808804740	keyphrase extraction
0.6774939413	raw frequency
0.6768168658	penalty
0.6758879091	length
0.6757395541	proposed
0.6743645996	information
0.6729511815	data
0.6729498909	training
0.6711602197	semantic
0.6709489651	dp
0.6694473924	valet
0.6692668764	precision
0.6690177220	topmine segphrase + topmine
0.6686738690	fig
0.6685560241	chunking
0.6683399137	dw bt
0.6682361221	′
0.6675238859	model
0.6672415851	interesting phrases mined
0.6669337486	θ
0.6650250520	web
0.6644986112	r0
0.6640296072	set
0.6633188428	relational
0.6627273716	based
0.6614402356	analytical processing
0.6608741033	frequency
0.6606520801	compared
0.6599158271	support
0.6584349903	finally
0.6584349903	iter
0.6570938593	research
0.6566585785	parking
0.6566561976	processing
0.6566552764	kea
0.6563598556	corpus
0.6557871663	word
0.6550681832	small
0.6543471478	rule
0.6536280147	classifier
0.6525215263	quality estimation
0.6510159236	feature
0.6501128599	learning
0.6498524859	table
0.6483396525	rectified
0.6478676610	bt
0.6458753146	rice
0.6452329868	segmentation
0.6450463381	language
0.6424431183	estimation
0.6422770935	extraction
0.6421670141	wiki phrases
0.6393965363	space
0.6386004210	corpora
0.6386004210	occurrences
0.6373498383	results
0.6364647081	academia dataset
0.6354119411	concordance
0.6346655899	statistical
0.6344911838	cd
0.6338710620	mutual
0.6336094439	performance
0.6333720938	δ
0.6325249277	li
0.6302108280	methods
0.6299576420	algorithm
0.6293196151	size
0.6292864630	dw
0.6292864630	introduced
0.6282618771	conference
0.6282618771	figure
0.6282618771	wikipedia
0.6269818650	dataset
0.6261712917	text
0.6254633162	similar
0.6249950253	big
0.6247519966	computational
0.6246923405	st
0.6244197874	quality
0.6232911805	context
0.6232911805	sequences
0.6232463486	maximum
0.6228410097	segment
0.6205449366	retrieval
0.6195103532	complexity
0.6193432063	datasets
0.6189064103	reported
0.6184191497	candidates
0.6181481772	evaluated
0.6176254690	database system
0.6167204061	recall
0.6156929774	frequent
0.6155333061	phrases
0.6140337871	classphrase segphrase segphrase +
0.6132354151	yelp dataset
0.6126516908	phrase
0.6124522747	phrase candidates
0.6117727683	soup
0.6112656099	phrase quality estimation
0.6107341579	dynamic
0.6102042283	complete
0.6102042283	specific
0.6094969122	search
0.6081507534	olap
0.6054824426	hash
0.6052247202	f1
0.6052247202	input
0.6048967514	proceedings
0.6043837863	user
0.6043837863	domain
0.6043837863	pattern
0.6042327125	shown
0.6025719061	automatic
0.6021376351	experimental
0.6020605120	text corpora
0.6019939541	conextr
0.6003017601	discovery
0.6003017601	automaton
0.6003017601	association
0.5990483573	ratio
0.5990483573	log
0.5984047287	io n
0.5980062464	defined
0.5978846711	bt + 1 − bt
0.5971607844	study
0.5954017601	total
0.5950814927	phrase quality
0.5948735914	←
0.5931477253	−
0.5929344800	self parking
0.5927421440	pooling
0.5926147856	curves
0.5920203816	quality phrases
0.5917920094	relational database system
0.5905967784	pointwise
0.5905598725	completeness
0.5905598725	punctuation
0.5905598725	smaller
0.5905598725	contributions
0.5905598725	accurate
0.5905598725	databases
0.5894672800	tf idf and c value
0.5892660200	words
0.5892307410	segmented
0.5881780241	ur
0.5879345754	specifically
0.5867592738	output
0.5866975128	rectified frequency
0.5843328663	word sequence
0.5842078041	θ ′
0.5795213830	segment length penalty
0.5789710032	method
0.5787547900	feedback
0.5778660114	interesting
0.5769762760	long
0.5763898532	runtime
0.5760243225	informativeness
0.5760243225	unsupervised
0.5756723910	collection
0.5750262920	previous
0.5750262920	score
0.5750262920	statistics
0.5748355423	interesting phrase mining
0.5748311620	features
0.5730208903	segphrase +
0.5729864280	+ 08
0.5729350590	raw
0.5707217552	segmented corpus
0.5702751981	query
0.5701715495	topic
0.5694919924	prefix
0.5683592110	precision recall curves
0.5670743799	iterations
0.5655536867	conducted
0.5653491220	analysis
0.5651418870	generate
0.5651418870	hard
0.5647996007	extracting
0.5642011476	case
0.5636751036	efficiently
0.5636751036	examples
0.5630796348	bt + 1
0.5613507516	high
0.5611270646	multidimensional
0.5611270646	estimate
0.5608775689	lower
0.5603996634	large
0.5590983794	components
0.5590983794	predicted
0.5590983794	covered
0.5590983794	iteration
0.5590983794	improve
0.5590983794	train
0.5590983794	subset
0.5590983794	compute
0.5590983794	expected
0.5590983794	partition
0.5590983794	estimated
0.5590983794	measure
0.5590983794	popular
0.5590983794	single
0.5590983794	correctly
0.5590983794	interpreted
0.5590983794	goal
0.5590983794	order
0.5590983794	ranks
0.5590983794	general
0.5590983794	provided
0.5590983794	tasks
0.5590983794	dimensional
0.5590983794	including
0.5590983794	collections
0.5569883594	slightly
0.5564159669	
0.5561341494	concept
0.5542171972	0.3 0.4 0.5 0.6 0.7 0.8
0.5507828122	false
0.5501108505	phrasal
0.5496399576	efficient
0.5494942357	distributed
0.5494349603	super
0.5491066772	quality estimator
0.5479281243	strong
0.5470178525	frequencies
0.5457302709	machine
0.5455057615	queries
0.5451457701	labeled
0.5445702038	evaluation
0.5424582258	integrated
0.5405483724	informative
0.5402699552	setting
0.5396808946	document
0.5370345033	result
0.5370345033	application
0.5370345033	important
0.5370345033	mentioned
0.5370345033	existing
0.5355997811	significant
0.5355997811	stopwords
0.5355997811	probability
0.5355997811	ranked
0.5355997811	techniques
0.5355997811	studies
0.5355997811	minimum
0.5355997811	longer
0.5355997811	concepts
0.5355997811	requires
0.5355997811	†
0.5355997811	parameters
0.5355997811	parameter
0.5355997811	dictionary
0.5355997811	return
0.5355997811	requirements
0.5355997811	occurrence
0.5355997811	rules
0.5344451035	experiments
0.5327856796	ranking
0.5327856796	segments
0.5327856796	constant
0.5327856796	higher
0.5327856796	i.e
0.5327856796	count
0.5327856796	comparison
0.5327856796	counts
0.5327856796	approaches
0.5327856796	task
0.5327856796	increases
0.5327856796	requirement
0.5327856796	lists
0.5319998902	index
0.5315202998	positive
0.5307973129	step
0.5286405124	automatically
0.5256611002	e.g
0.5254350166	ul
0.5253762422	computed
0.5250635292	number
0.5248542703	list
0.5248535065	low
0.5243920279	meaningful
0.5240485073	significantly
0.5220540760	baselines
0.5220540760	∑
0.5220540760	terms
0.5220540760	efficiency
0.5220540760	observe
0.5220540760	models
0.5220540760	instance
0.5220540760	required
0.5220540760	problem
0.5220540760	units
0.5210304177	α
0.5203045018	provide
0.5191323105	documents
0.5181189919	–
0.5178548417	quality assessment
0.5169085862	candidate
0.5169085862	ω
0.5169085862	linear
0.5166083570	phrase mining
0.5160394448	process
0.5149935184	support vector
0.5148981863	framework
0.5148329167	∈
0.5147084564	generated
0.5145572539	approach
0.5140212941	normalized
0.5121119904	perform
0.5121119904	propose
0.5119303528	applications
0.5106133597	similar words
0.5104997806	parts
0.5073933364	# wiki phr
0.5058136192	·
0.5058059395	pages
0.4906736657	corpus size
0.4771124367	u ←
0.4723874820	p re c
0.4719619555	chunking method
0.4661801967	phrase extraction
0.4654761098	based on
0.4585897333	phrase quality assessment
0.4546012981	academia yelp
0.4495391443	| s |
0.4420711960	rely on
0.4406708158	phrase length
0.4396247836	| st |
0.4383382290	+ 1
0.4285640435	non segmented ratio
0.4271963487	0.75 0.8 0.85 0.9 0.95
0.4258392698	− 1
0.4093889558	precision recall curves on
0.4086332645	′ u
0.4079654784	+ δ
0.4070736214	# total
0.3891742545	q
0.3879985977	this paper
0.3870173165	training labels
0.3869050149	word sequences
0.3834328205	w bt
0.3799955173	c
0.3797519472	segmentation features
0.3786706500	y
0.3780868605	l
0.3753695612	refers to
0.3751959285	d = 1
0.3723127886	log p
0.3717693129	= bt +
0.3617668427	interesting phrases
0.3607637611	t
0.3605522367	| c |
0.3592977541	non segmented ratio r0
0.3560905505	time and space
0.3546189976	j
0.3517894623	first
0.3501382296	instead
0.3477731454	evaluated by
0.3472922325	does not
0.3449560379	example
0.3432397526	frequent phrases
0.3399730031	system
0.3377169087	meanwhile
0.3356854506	0.6
0.3344911277	s
0.3330470887	g
0.3328259354	0.5
0.3324815264	w
0.3322156640	part
0.3316477722	few
0.3295210046	mining
0.3263146750	however
0.3254745970	number of
0.3253441397	shown in
0.3227010507	b
0.3169275686	non
0.3165346814	#
0.3144218557	h
0.3141964533	0.95
0.3141964533	0.9
0.3139365090	0.7
0.3126117680	above
0.3123063391	whole
0.3120918377	d
0.3120064032	o
0.3114229916	r
0.3107992937	re
0.3105966270	0.2
0.3083834285	0.85
0.3078831667	here
0.3074068318	work
0.3068237800	the appendix
0.3053333167	method segphrase +
0.3044195795	i ←
0.3038873118	well
0.3025384503	u
0.3005305880	value
0.2976540714	this
0.2975559543	good
0.2970306204	therefore
0.2965898130	e
0.2964533979	a
0.2934310989	t = 1
0.2915666673	following
0.2907158064	k
0.2895708416	i
0.2883109377	n
0.2867752813	u ′
0.2867667587	due to
0.2858808896	second
0.2855042456	+
0.2846089200	20
0.2820363562	1
0.2816432517	number of labels
0.2808724367	|
0.2801679562	0.2 0.4 0.6 0.8
0.2779501419	bottom
0.2771087365	=
0.2758837110	0
0.2755573756	m
0.2747339811	under
0.2730436264	p
0.2728174708	f
0.2728034084	do
0.2725367418	time
0.2713675492	such
0.2713390584	proposed methods
0.2710667542	through
0.2702636505	a phrase
0.2696087623	and
0.2682868520	thus
0.2681617499	example 2
0.2676452319	reported in
0.2671195442	than
0.2669081706	on
0.2662666418	5
0.2659131584	within
0.2643448358	top
0.2635838846	since
0.2633281274	c value
0.2617518802	not
0.2616444806	several
0.2606783564	the quality of
0.2596399558	150
0.2596143664	according to
0.2590706875	there
0.2590438072	when
0.2582613732	so
0.2574284817	mined from
0.2566141257	@
0.2566076433	in acl
0.2561785406	very
0.2554798408	may
0.2549141564	how
0.2546632770	u |
0.2543695525	even
0.2540904897	another
0.2521504926	in
0.2516067806	as
0.2496039963	whether
0.2495386088	need to
0.2494757764	no
0.2493506680	last
0.2480404472	29
0.2473799457	among
0.2454995963	2000
0.2454995963	see
0.2454995963	was
0.2454995963	30
0.2454995963	various
0.2454995963	down
0.2454995963	hi
0.2454995963	17
0.2454995963	possible
0.2454995963	12
0.2454995963	sub
0.2454995963	9
0.2454995963	been
0.2453007056	by
0.2447864109	the
0.2443725950	if
0.2441771908	of
0.2428816545	v
0.2425306405	then
0.2424394664	15
0.2421366277	2008
0.2421366277	via
0.2421366277	4.2
0.2421366277	10
0.2421366277	too
0.2420879519	2
0.2419141562	0.948
0.2400437353	while
0.2398511181	some
0.2397894708	it
0.2394294886	about
0.2394294886	appear
0.2394294886	yes
0.2394294886	now
0.2392818401	they
0.2389529621	find
0.2389529621	300
0.2389529621	200
0.2389529621	make
0.2389529621	8
0.2385502087	might
0.2376352708	at
0.2358847921	to
0.2357333911	after
0.2347078595	2010
0.2339106329	with
0.2333959608	over
0.2331384430	from
0.2329327949	most
0.2329060909	1000
0.2329060909	13
0.2327107063	only
0.2316011075	were
0.2302423187	other
0.2299808855	because
0.2299208525	less
0.2294956175	up
0.2294620627	4
0.2291646238	more
0.2282881084	given
0.2280646763	defined as
0.2278863838	for
0.2276664234	use
0.2276664234	could
0.2276606094	many
0.2276606094	like
0.2275828086	certain
0.2275828086	7
0.2267118957	them
0.2267118957	will
0.2262455153	where
0.2262455153	used
0.2262455153	better
0.2262455153	show
0.2262455153	further
0.2262278095	also
0.2262278095	these
0.2261934790	each
0.2261231481	we
0.2260129023	6
0.2259042004	best
0.2250437120	way
0.2243547528	into
0.2240349030	different
0.2233870251	one
0.2231655418	our
0.2227638437	between
0.2226682756	new
0.2226377390	an
0.2220709489	using
0.2219662690	has
0.2219652845	the number of
0.2216511799	its
0.2214786261	two
0.2212510232	their
0.2193790594	both
0.2185129683	3
0.2155668785	but
0.2155042877	or
0.2154885476	which
0.2152639636	all
0.2146012931	should
0.2140106168	can
0.2134272835	be
0.2130971515	that
0.2125885412	have
0.2117476513	is
0.2117476513	are
0.2093089654	method segphrase
0.2080841944	query data
0.1923487090	frequent phrase
0.1875800223	the total
0.1872581886	phrases mined from
0.1869340364	phrase chunking
0.1843693811	1 tf idf
0.1819124103	topmine segphrase +
0.1770587343	corpus c
0.1731976110	segphrase segphrase +
0.1645543589	a few
0.1562553291	do not
0.1527245134	a whole
0.1454458975	phrases with
0.1436911346	length penalty
0.1389442380	c i
0.1382448470	instead of
0.1382021779	such as
0.1341503304	the first
0.1278490263	a word sequence
0.1220128268	segmentation results
0.1174703032	= 1 to
0.1154624453	a sequence
0.1137124453	mining data
0.1107520378	segmentation s
0.1105133556	good phrase
0.1061823597	the raw frequency
0.1034478244	on yelp dataset
0.1033926274	in table
0.1026063316	for example
0.1006649896	occurrences of
0.0983926274	the corpus
0.0947064166	precision recall
0.0901384745	introduced in
0.0874478244	on academia dataset
0.0866733116	as well
0.0863335237	bt +
0.0839772114	# wiki
0.0838875973	compared to
0.0825077565	the phrase quality
0.0787201603	the following
0.0752701484	this work
0.0747207786	a corpus
0.0721189112	in sec
0.0718207786	the segmentation
0.0702232213	the phrasal segmentation
0.0699966890	a small
0.0692282916	quality phrase
0.0690332367	in alg
0.0663335237	performance of
0.0659207786	the phrase
0.0657775713	| =
0.0644651444	of segphrase +
0.0612244231	of quality phrases
0.0609450203	set of
0.0575557459	phrases in
0.0528890793	phrases from
0.0503425766	c |
0.0501224126	quality of
0.0419696378	table 1
0.0396224126	results in
0.0380846675	= 1
0.0374450203	quality q
0.0347207786	a word
0.0338322752	the classifier
0.0315675766	by segphrase +
0.0296609047	t =
0.0293207786	the precision
0.0287057345	= p
0.0282498190	a segmentation
0.0273207786	the concordance
0.0255430008	the context
0.0245430008	the chunking
0.0244831269	the above
0.0239430008	and segphrase
0.0235430008	the frequency
0.0231057345	and j
0.0211164857	in fig
0.0204942380	1 to
0.0185232784	time and
0.0179390679	and c
0.0157072752	the number
0.0138140679	a good
0.0126057345	| s
