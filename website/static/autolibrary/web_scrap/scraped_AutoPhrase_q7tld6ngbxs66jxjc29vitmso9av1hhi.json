[{"link": "http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8306825", "title": "Automated Phrase Mining from Massive Text Corpora", "authors": [{"firstName": "Jingbo", "middleNames": [], "lastName": "Shang"}, {"firstName": "Jialu", "middleNames": [], "lastName": "Liu"}, {"firstName": "Meng", "middleNames": [], "lastName": "Jiang"}, {"firstName": "X.", "middleNames": [], "lastName": "Ren"}, {"firstName": "Clare", "middleNames": ["R."], "lastName": "Voss"}, {"firstName": "Jiawei", "middleNames": [], "lastName": "Han"}], "abstract": "As one of the fundamental tasks in text analysis, phrase mining aims at extracting quality phrases from a text corpus and has various downstream applications including information extraction/retrieval, taxonomy construction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, and thus likely have unsatisfactory performance on text corpora of new domains and genres without extra but expensive adaption. None of the state-of-the-art models, even data-driven models, is fully automated because they require human experts for designing rules or labeling phrases. In this paper, we propose a novel framework for automated phrase mining, <inline-formula> <tex-math notation=\"LaTeX\">$\\mathsf{AutoPhrase}$</tex-math><alternatives> <inline-graphic xlink:href=\"shang-ieq1-2812203.gif\"/></alternatives></inline-formula>, which supports any language as long as a general knowledge base (e.g., Wikipedia) in that language is available, while benefiting from, but not requiring, a POS tagger. Compared to the state-of-the-art methods, <inline-formula><tex-math notation=\"LaTeX\"> $\\mathsf{AutoPhrase}$</tex-math><alternatives><inline-graphic xlink:href=\"shang-ieq2-2812203.gif\"/></alternatives> </inline-formula> has shown significant improvements in both effectiveness and efficiency on five real-world datasets across different domains and languages. Besides, <inline-formula><tex-math notation=\"LaTeX\">$\\mathsf{AutoPhrase}$ </tex-math><alternatives><inline-graphic xlink:href=\"shang-ieq3-2812203.gif\"/></alternatives></inline-formula> can be extended to model single-word quality phrases.", "date": "2017-02-15"}, {"link": "https://pdfs.semanticscholar.org/cf20/7e1df031ec745c094991ec29b4bbbd2161b3.pdf", "title": "N-ary relation extraction for simultaneous T-Box and A-Box knowledge base augmentation", "authors": [{"firstName": "M.", "middleNames": [], "lastName": "Fossati"}, {"firstName": "Emilio", "middleNames": [], "lastName": "Dorigatti"}, {"firstName": "C.", "middleNames": [], "lastName": "Giuliano"}], "abstract": "The Web has evolved into a huge mine of knowledge carved in different forms, the predominant one still being the free-text document. This motivates the need for Intelligent Web-reading Agents: hypothetically, they would skim through disparate Web sources corpora and generate meaningful structured assertions to fuel Knowledge Bases (KBs). Ultimately, comprehensive KBs, like WIKIDATA and DBPEDIA, play a fundamental role to cope with the issue of information overload. On account of such vision, this paper depicts the FACT EXTRACTOR, a complete Natural Language Processing (NLP) pipeline which reads an input textual corpus and produces machine-readable statements. Each statement is supplied with a confidence score and undergoes a disambiguation step via Entity Linking, thus allowing the assignment of KB-compliant URIs. The system implements four research contributions: it (1) executes N-ary relation extraction by applying the Frame Semantics linguistic theory, as opposed to binary techniques; it (2) simultaneously populates both the T-Box and the A-Box of the target KB; it (3) relies on a single NLP layer, namely part-of-speech tagging; it (4) enables a completely supervised yet reasonably priced machine learning environment through a crowdsourcing strategy. We assess our approach by setting the target KB to DBpedia and by considering a use case of 52, 000 Italian Wikipedia soccer player articles. Out of those, we yield a dataset of more than 213, 000 triples with an estimated 81.27% F1. We corroborate the evaluation via (i) a performance comparison with a baseline system, as well as (ii) an analysis of the T-Box and A-Box augmentation capabilities. The outcomes are incorporated into the Italian DBpedia chapter, can be queried through its SPARQL endpoint, and/or downloaded as standalone data dumps. The codebase is released as free software and is publicly available in the DBpedia Association repository."}, {"link": "https://www.aclweb.org/anthology/L18-1031.pdf", "title": "When ACE met KBP: End-to-End Evaluation of Knowledge Base Population with Component-level Annotation", "authors": [{"firstName": "B.", "middleNames": [], "lastName": "Min"}, {"firstName": "M.", "middleNames": [], "lastName": "Freedman"}, {"firstName": "Roger", "middleNames": [], "lastName": "Bock"}, {"firstName": "R.", "middleNames": [], "lastName": "Weischedel"}], "abstract": "Building a Knowledge Base from text corpora is useful for many applications such as question answering and web search. Since 2012, the Cold Start Knowledge Base Population (KBP) evaluation at the Text Analysis Conference (TAC) has attracted many participants. Despite the popularity, the Cold Start KBP evaluation has several problems including but not limited to the following two: first, each year\u2019s assessment dataset is a pooled set of query-answer pairs, primarily generated by participating systems. It is well known to participants that there is pooling bias: a system developed outside of the official evaluation period is not rewarded for finding novel answers, but rather is penalized for doing so. Second, the assessment dataset, constructed with lots of human effort, offers little help in training information extraction algorithms which are crucial ingredients for the end-to-end KBP task. To address these problems, we propose a new unbiased evaluation methodology that uses existing component-level annotation such as the Automatic Content Extraction (ACE) dataset, to evaluate Cold Start KBP. We also propose bootstrap resampling to provide statistical significance to the results reported. We will then present experimental results and analysis.", "date": "2018-05-01"}, {"link": "https://pdfs.semanticscholar.org/4b1e/6f34e9570376224cfd82655b83ca4909d825.pdf", "title": "Design and Construction of a NLP Based Knowledge Extraction Methodology in the Medical Domain Applied to Clinical Information", "authors": [{"firstName": "Denis", "middleNames": ["Cede\u00f1o"], "lastName": "Moreno"}, {"firstName": "Miguel", "middleNames": [], "lastName": "Vargas-Lombardo"}], "abstract": "Objectives This research presents the design and development of a software architecture using natural language processing tools and the use of an ontology of knowledge as a knowledge base. Methods The software extracts, manages and represents the knowledge of a text in natural language. A corpus of more than 200 medical domain documents from the general medicine and palliative care areas was validated, demonstrating relevant knowledge elements for physicians. Results Indicators for precision, recall and F-measure were applied. An ontology was created called the knowledge elements of the medical domain to manipulate patient information, which can be read or accessed from any other software platform. Conclusions The developed software architecture extracts the medical knowledge of the clinical histories of patients from two different corpora. The architecture was validated using the metrics of information extraction systems.", "date": "2018-10-01"}, {"link": "https://doi.org/10.1007/978-3-030-34058-2_24", "title": "Weakly-Supervised Relation Extraction in Legal Knowledge Bases", "authors": [{"firstName": "Haojie", "middleNames": [], "lastName": "Huang"}, {"firstName": "Raymond", "middleNames": ["K."], "lastName": "Wong"}, {"firstName": "Baoxiang", "middleNames": [], "lastName": "Du"}, {"firstName": "Hae", "middleNames": ["Jin"], "lastName": "Han"}], "abstract": "Intelligent legal information systems are becoming popular recently. Relation extraction in massive legal text corpora such as precedence cases is essential for building knowledge bases behind these systems. Recently, most works have applied deep learning to identify relations between entities in text. However, they require a large amount of human labelling, which is labour intensive and expensive in the legal field. This paper proposes a novel method to effectively extract relations from legal precedence cases. In particular, relation feature embeddings are trained in an unsupervised way. With limited labelled data, the proposed method is shown to effective in constructing a legal knowledge base.", "date": "2019-11-04"}]