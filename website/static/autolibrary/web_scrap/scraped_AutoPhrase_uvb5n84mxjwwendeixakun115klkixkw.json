[{"link": "http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8955788", "title": "DBGE: Employee Turnover Prediction Based on Dynamic Bipartite Graph Embedding", "authors": [{"firstName": "Xinjun", "middleNames": [], "lastName": "Cai"}, {"firstName": "Jiaxing", "middleNames": [], "lastName": "Shang"}, {"firstName": "Z.", "middleNames": [], "lastName": "Jin"}, {"firstName": "Feiyi", "middleNames": [], "lastName": "Liu"}, {"firstName": "Baohua", "middleNames": [], "lastName": "Qiang"}, {"firstName": "W.", "middleNames": [], "lastName": "Xie"}, {"firstName": "Liang", "middleNames": [], "lastName": "Zhao"}], "abstract": "The issue of employee turnover is always critical for companies, and accurate predictions can help them prepare in time. Most past studies on employee turnover have focused on analyzing impact factors or using simple network centrality measures. In this paper, we study the problem from a completely new perspective by modeling users\u2019 historical job records as a dynamic bipartite graph. Specifically, we propose a bipartite graph embedding method with temporal information called dynamic bipartite graph embedding (DBGE) to learn the vector representation of employees and companies. Our approach not only considers the relations between employees and companies but also incorporates temporal information embedded in consecutive work records. We first define the Horary Random Walk on a bipartite graph to generate a sequence for each vertex in chronological order. Then, we employ the skip-gram model to obtain a temporal low-dimensional vector representation for each vertex and apply machine learning methods to predict employee turnover behavior by combining embedded features with employees\u2019 basic information. Experiments on a real-world dataset collected from one of China\u2019s largest online professional social networks show that the features learned through DBGE can significantly improve turnover prediction performance. Moreover, experiments on public Amazon and Taobao datasets show that our approach achieves better performance in the link prediction and visualization task than other graph embedding methods that do not consider temporal information.", "date": "2020-01-10"}, {"link": "https://arxiv.org/pdf/1909.03359.pdf", "title": "Distributed Training of Embeddings using Graph Analytics", "authors": [{"firstName": "G.", "middleNames": [], "lastName": "Gill"}, {"firstName": "Roshan", "middleNames": [], "lastName": "Dathathri"}, {"firstName": "S.", "middleNames": [], "lastName": "Maleki"}, {"firstName": "Madan", "middleNames": [], "lastName": "Musuvathi"}, {"firstName": "Todd", "middleNames": [], "lastName": "Mytkowicz"}, {"firstName": "Olli", "middleNames": ["Saarikivi", "The", "University", "of", "Texas", "at"], "lastName": "Austin"}, {"firstName": "M.", "middleNames": [], "lastName": "Research"}], "abstract": "Many applications today, such as NLP, network analysis, and code analysis, rely on semantically embedding objects into low-dimensional fixed-length vectors. Such embeddings naturally provide a way to perform useful downstream tasks, such as identifying relations among objects or predicting objects for a given context, etc. Unfortunately, the training necessary for accurate embeddings is usually computationally intensive and requires processing large amounts of data. Furthermore, distributing this training is challenging. Most embedding training uses stochastic gradient descent (SGD), an \"inherently\" sequential algorithm. Prior approaches to parallelizing SGD do not honor these dependencies and thus potentially suffer poor convergence. \nThis paper presents a distributed training framework for a class of applications that use Skip-gram-like models to generate embeddings. We call this class Any2Vec and it includes Word2Vec, DeepWalk, and Node2Vec among others. We first formulate Any2Vec training algorithm as a graph application and leverage the state-of-the-art distributed graph analytics framework, D-Galois. We adapt D-Galois to support dynamic graph generation and repartitioning, and incorporate novel communication optimizations. Finally, we introduce a novel way to combine gradients during distributed training to prevent accuracy loss. We show that our framework, called GraphAny2Vec, matches on a cluster of 32 hosts the accuracy of the state-of-the-art shared-memory implementations of Word2Vec and Vertex2Vec on 1 host, and gives a geo-mean speedup of 12x and 5x respectively. Furthermore, GraphAny2Vec is on average 2x faster than the state-of-the-art distributed Word2Vec implementation, DMTK, on 32 hosts. We also show the superiority of our Gradient Combiner independent of GraphAny2Vec by incorporating it in DMTK, which raises its accuracy by > 30%.", "date": "2019-09-08"}, {"link": "http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594938", "title": "Pseudo-Implicit Feedback for Alleviating Data Sparsity in Top-K Recommendation", "authors": [{"firstName": "Yun", "middleNames": [], "lastName": "He"}, {"firstName": "Haochen", "middleNames": [], "lastName": "Chen"}, {"firstName": "Ziwei", "middleNames": [], "lastName": "Zhu"}, {"firstName": "James", "middleNames": [], "lastName": "Caverlee"}], "abstract": "We propose PsiRec, a novel user preference propagation recommender that incorporates pseudo-implicit feedback for enriching the original sparse implicit feedback dataset. Three of the unique characteristics of PsiRec are: (i) it views user-item interactions as a bipartite graph and models pseudo-implicit feedback from this perspective; (ii) its random walks-based approach extracts graph structure information from this bipartite graph, toward estimating pseudo-implicit feedback; and (iii) it adopts a Skip-gram inspired measure of confidence in pseudo-implicit feedback that captures the pointwise mutual information between users and items. This pseudo-implicit feedback is ultimately incorporated into a new latent factor model to estimate user preference in cases of extreme sparsity. PsiRec results in improvements of 21.5% and 22.7% in terms of Precision@10 and Recall@10 over state-of-the-art Collaborative Denoising Auto-Encoders. Our implementation is available at https://github.com/heyunh2015/PsiRecICDM2018.", "date": "2018-11-01"}]