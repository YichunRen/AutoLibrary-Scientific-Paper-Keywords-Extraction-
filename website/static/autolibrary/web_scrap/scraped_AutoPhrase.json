[{"link": "http://dl.acm.org/citation.cfm?id=3160604", "title": "On the Power of Massive Text Data", "authors": [{"firstName": "Jiawei", "middleNames": [], "lastName": "Han"}], "abstract": "The real-world big data is largely unstructured, dynamic, and interconnected, in the form of natural language text. It is highly desirable to transform such massive unstructured data into structured knowledge. Many researchers and practitioners rely on labor-intensive labeling and curation to extract knowledge from unstructured text data. However, such approaches may not be scalable to web-scale or adaptable to new domains, especially considering that a lot of text corpora are highly dynamic and domain-specific. We argue that massive text data itself contains a large body of hidden patterns, structures, and knowledge. Equipped with domain-independent and domain-specific knowledge-bases, a promising direction is to develop more systematic data mining methods to turn massive unstructured text data into structured knowledge. We introduce a set of methods developed recently in our own group on exploration of the power of big text data, including mining quality phrases using unsupervised, weakly supervised and distantly supervised approaches, recognition and typing of entities and relations by distant supervision, meta-pattern-based entity-attribute-value extraction, set expansion and local embedding-based multi-faceted taxonomy discovery, allocation of text documents into multi-dimensional text cubes, construction of heterogeneous information networks from text cube, and eventually mining multi-dimensional structured knowledge from massive text data. We show that massive text data itself can be powerful at disclosing patterns, structures and hidden knowledge, and it is promising to explore the power of massive, interrelated text data for transforming such unstructured data into structured knowledge.", "date": "2018-02-02"}, {"link": "https://arxiv.org/pdf/1912.01731.pdf", "title": "HAMNER: Headword Amplified Multi-span Distantly Supervised Method for Domain Specific Named Entity Recognition", "authors": [{"firstName": "Shaoweihua", "middleNames": [], "lastName": "Liu"}, {"firstName": "Y.", "middleNames": [], "lastName": "Sun"}, {"firstName": "Bing", "middleNames": [], "lastName": "Li"}, {"firstName": "Wei", "middleNames": [], "lastName": "Wang"}, {"firstName": "X.", "middleNames": [], "lastName": "Zhao"}], "abstract": "To tackle Named Entity Recognition (NER) tasks, supervised methods need to obtain sufficient cleanly annotated data, which is labor and time consuming. On the contrary, distantly supervised methods acquire automatically annotated data using dictionaries to alleviate this requirement. Unfortunately, dictionaries hinder the effectiveness of distantly supervised methods for NER due to its limited coverage, especially in specific domains. In this paper, we aim at the limitations of the dictionary usage and mention boundary detection. We generalize the distant supervision by extending the dictionary with headword based non-exact matching. We apply a function to better weight the matched entity mentions. We propose a span-level model, which classifies all the possible spans then infers the selected spans with a proposed dynamic programming algorithm. Experiments on all three benchmark datasets demonstrate that our method outperforms previous state-of-the-art distantly supervised methods.", "date": "2019-12-03"}, {"link": "https://arxiv.org/pdf/2007.15779.pdf", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing", "authors": [{"firstName": "Yu", "middleNames": [], "lastName": "Gu"}, {"firstName": "Robert", "middleNames": [], "lastName": "Tinn"}, {"firstName": "Hao", "middleNames": [], "lastName": "Cheng"}, {"firstName": "M.", "middleNames": [], "lastName": "Lucas"}, {"firstName": "Naoto", "middleNames": [], "lastName": "Usuyama"}, {"firstName": "Xiaodong", "middleNames": [], "lastName": "Liu"}, {"firstName": "Tristan", "middleNames": [], "lastName": "Naumann"}, {"firstName": "Jianfeng", "middleNames": [], "lastName": "Gao"}, {"firstName": "Hoifung", "middleNames": [], "lastName": "Poon"}], "abstract": "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at this https URL.", "date": "2020-07-31"}, {"link": "https://www.aclweb.org/anthology/D19-1546.pdf", "title": "JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation", "authors": [{"firstName": "R.", "middleNames": [], "lastName": "Agashe"}, {"firstName": "Srini", "middleNames": [], "lastName": "Iyer"}, {"firstName": "Luke", "middleNames": [], "lastName": "Zettlemoyer"}], "abstract": "Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks: (1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.", "date": "2019-10-01"}, {"link": "https://arxiv.org/pdf/2004.03554.pdf", "title": "Fine-Grained Named Entity Typing over Distantly Supervised Data Based on Refined Representations", "authors": [{"firstName": "M.", "middleNames": ["A."], "lastName": "Ali"}, {"firstName": "Y.", "middleNames": [], "lastName": "Sun"}, {"firstName": "Bing", "middleNames": [], "lastName": "Li"}, {"firstName": "Wei", "middleNames": [], "lastName": "Wang"}], "abstract": "Fine-Grained Named Entity Typing (FG-NET) is a key component in Natural Language Processing (NLP). It aims at classifying an entity mention into a wide range of entity types. Due to a large number of entity types, distant supervision is used to collect training data for this task, which noisily assigns type labels to entity mentions irrespective of the context. In order to alleviate the noisy labels, existing approaches on FG-NET analyze the entity mentions entirely independent of each other and assign type labels solely based on mention's sentence-specific context. This is inadequate for highly overlapping and/or noisy type labels as it hinders information passing across sentence boundaries. For this, we propose an edge-weighted attentive graph convolution network that refines the noisy mention representations by attending over corpus-level contextual clues prior to the end classification. Experimental evaluation shows that the proposed model outperforms the existing research by a relative score of upto 10.2% and 8.3% for macro-f1 and micro-f1 respectively.", "date": "2020-04-03"}]